{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression via Stochastic Gradient Ascent\n",
    "\n",
    "The goal of this notebook is to implement a logistic regression classifier using stochastic gradient ascent. You will:\n",
    "\n",
    " * Extract features from Amazon product reviews.\n",
    " * Convert an SFrame into a NumPy array.\n",
    " * Write a function to compute the derivative of log likelihood function (with L2 penalty) with respect to a single coefficient.\n",
    " * Implement stochastic gradient ascent with L2 penalty\n",
    " * Compare convergence of stochastic gradient ascent with that of batch gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up graphlab create\n",
    "\n",
    "Make sure you have the latest version of GraphLab Create. If you don't find the decision tree module, then you would need to upgrade graphlab-create using\n",
    "\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import graphlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, we will use a subset of the Amazon product review dataset. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews. (**Add link to the subset data**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = graphlab.SFrame('amazon_baby_subset.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did in an earlier assignment, we will work with a subset of hand-curated **important** words. We perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string manipulation functionality.\n",
    "2. Compute word counts (only for the important_words)\n",
    "\n",
    "Refer to notes in the earlier assignments for more details on how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: \n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]\n",
    "\n",
    "# Remote punctuation\n",
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "\n",
    "# Split out the words into individual columns\n",
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The SFrame *products* now contains one column for each of the 193 **important_words**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "We will now split the data into a 90-10 split where 90% is in the training set and 10% is in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.9, seed=1)\n",
    "\n",
    "print 'Training set : %d data points' % len(train_data)\n",
    "print 'Test set     : %d data points' % len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SFrame to NumPy array\n",
    "\n",
    "Just like in the earlier assignments, we provide you with a function that extracts columns from an SFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels. \n",
    "\n",
    "**Note:** The feature matrix includes an additional column 'intercept' filled with 1's to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we convert both training and test sets into NumPy arrays.\n",
    "\n",
    "**Warning**: This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_test, sentiment_test = get_numpy_data(test_data, important_words, 'sentiment') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz question**: In the earlier assignment, there were 194 features (an intercept + one feature for each of the 193 important words). In this assignment, we will use stochastic-gradient ascent to train the classifier using logistic regression. How does the changing the solver from to stochastic-gradient ascent effect the number of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on logistic regression with L2 penalty assignment\n",
    "\n",
    "Let us now build on previous assignments. Recall from lecture that the link function for logistic regression can be defined as:\n",
    "\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ is given by the word counts of **important_words** in the review $\\mathbf{x}_i$. \n",
    "\n",
    "\n",
    "We will use the **same code** as in this past assignment to make probability predictions since this part is not affected by using stochastic-gradient ascent as a solver. (Only the way in which the coefficients are learned is affected by using stochastic-gradient ascent as a solver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1. / (1.+np.exp(-score))    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative for a single feature\n",
    "\n",
    "Let us now work on making minor changes to how the derivative computation is performed for logistic regression with an L2 penalty. \n",
    "\n",
    "Recall from lecture and the previous assignment that for logistic regression with an L2 penalty, **per-coefficient derivative for logistic regression with an L2 penalty** is as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right) - 2\\lambda w_j\n",
    "$$\n",
    "and for the intercept term, we have\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_0} = \\sum_{i=1}^N h_0(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "Recall from an earlier assignment that \n",
    "we can compute the derivative of log likelihood with respect to a single coefficient $w_j$ by writing a function \n",
    "which accepts the following five arguments:\n",
    " * `errors` vector containing $(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}))$ for all $i$\n",
    " * `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$\n",
    " * `coefficient` containing the current value of coefficient $w_j$.\n",
    " * `l2_penalty` representing the L2 penalty constant $\\lambda$\n",
    " * `feature_is_constant` telling whether the $j$-th feature is constant or not.\n",
    " \n",
    "Complete the following code block which computes the feature derivative with an L2 penality given the above 5 terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "    \n",
    "    # Compute the dot product of errors and feature\n",
    "    ## YOUR CODE HERE\n",
    "    derivative = ...\n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant: \n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of the gradient computation, we provide a function for computing average log likelihood (which we recall from the last assignment was a topic detailed in an advanced optional video, and used here for its numerical stability).\n",
    "\n",
    "To track the performance of stochastic-gradient ascent, we provide a function for computing $\\color{red}{\\mbox{average}}$ log likelihood. \n",
    "\n",
    "$$\\ell\\ell_A(\\mathbf{w}) = \\color{red}{\\frac{1}{N}} \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) - \\lambda\\|\\mathbf{w}\\|_2^2 $$\n",
    "\n",
    "**Note** that we made one tiny modification to the log-likelihood function (called **compute_log_likelihood_with_L2**) in our earlier assignments. We added a $\\color{red}{\\frac{1}{N}}$ term which averages the log-likelihood accross all data points. The $\\color{red}{\\frac{1}{N}}$ term makes it easier for us to compare stochastic-gradient ascent with gradient ascent. We will use this function to generate plots that are similar to those you saw in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_avg_log_likelihood_with_l2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    \n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)/len(feature_matrix) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** Quiz question:** Recall from the lecture and the earlier assignment, the log-likelhood (wihout the averaging term) is given by \n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) -{\\lambda\\|\\mathbf{w}\\|_2^2} $$\n",
    "\n",
    "If the L2 regualarization $\\lambda=0$, how are the functions $\\ell\\ell(\\mathbf{w})$ and $\\ell\\ell_A(\\mathbf{w})$ related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the derivative for stochastic-gradient ascent\n",
    "\n",
    "Recall from the lecture that the gradient with respect to a single data point $\\color{red}{x_i}$ can be computed using the following formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})}{\\partial w_j} = \\sum_{i=1}^B h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right) - 2\\lambda w_j\n",
    "$$\n",
    "\n",
    "\n",
    "** Computing the gradient for a single data point**\n",
    "\n",
    "Do we really need to re-write all our code to modify $\\frac{\\partial\\ell(\\mathbf{w})}{\\partial w_j}$ to $\\frac{\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})}{{\\partial w_j}}$? \n",
    "\n",
    "\n",
    "Thankfully **No!**. Using NumPy, we an access $x_i$ in the training data using `feature_matrix_train[i:i+1,:]`\n",
    "and $y_i$ in the training data using `sentiment[i:i+1]`. We can compute $\\frac{\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})}{\\partial w_j}$ by re-using **all the code** written in `feature_derivative_with_L2` and `predict_probability`.\n",
    "\n",
    "\n",
    "We can compute $\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})$ easily using the following steps:\n",
    "* First, compute $P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ using the `predict_probability` function with `feature_matrix_train[i:i+1,:]` as the first parameter.\n",
    "* Next, compute $\\mathbf{1}[y_i = +1]$ using `sentiment_train[i:i+1]`.\n",
    "* Finally, call the `feature_derivative_with_L2` function with `feature_matrix_train[i:i+1, j]` as one of the parameters. \n",
    "\n",
    "Let us follow these steps for `j = 1` and `i = 10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 1                        # Feature number\n",
    "i = 10                       # Data point number\n",
    "coefficients = np.zeros(194) # A point w at which we are computing the gradient.\n",
    "l2_penalty = 0.0             # L2 penalty\n",
    "\n",
    "predictions = predict_probability(feature_matrix_train[i:i+1,:], coefficients)\n",
    "indicator = (sentiment_train[i:i+1]==+1)\n",
    "\n",
    "errors = indicator - predictions        \n",
    "gradient_single_data_point = feature_derivative_with_L2(errors, feature_matrix_train[i:i+1, j], \n",
    "                                    coefficients[j], l2_penalty, False)\n",
    "print \"Gradient single data point: %s\" % gradient_single_data_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** The code block above computed $\\frac{\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})}{{\\partial w_j}}$ for `j = 1` and `i = 10`.  Is $\\frac{\\partial\\ell_{\\color{red}{i}}(\\mathbf{w})}{{\\partial w_j}}$ a scalar or a 194-dimensional vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the derivative for using a batch of data points\n",
    "\n",
    "Given a mini-batch (or a set of data points) $x_{i}, x_{i+1} \\ldots x_{a+B}$, the gradient function for the batch of data points is given by:\n",
    "$$\n",
    "\\color{red}{\\sum_{s = i}^{i+B}} \\frac{\\partial\\ell_{s}}{\\partial w_j} = \\color{red}{\\sum_{s = i}^{i + B}} h_j(\\mathbf{x}_s)\\left(\\mathbf{1}[y_s = +1] - P(y_s = +1 | \\mathbf{x}_s, \\mathbf{w})\\right) - 2\\lambda w_j\n",
    "$$\n",
    "\n",
    "\n",
    "** Computing the gradient for a \"mini-batch\" of data points**\n",
    "Using NumPy, we an access the points $x_i, x_{i+1} \\ldots x_{i+B}$ in the training data using `feature_matrix_train[i:i+B,:]`\n",
    "and $y_i$ in the training data using `sentiment[i:i+B]`. \n",
    "\n",
    "We can compute $\\color{red}{\\sum_{s = i}^{i+B}} \\frac{\\partial\\ell_{s}}{\\partial w_j}$ easily as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = 1                        # Feature number\n",
    "i = 10                       # Data point start\n",
    "B = 10                       # Mini-batch size\n",
    "coefficients = np.zeros(194) # A point w at which we are computing the gradient.\n",
    "l2_penalty = 0.0             # L2 penalty\n",
    "\n",
    "predictions = predict_probability(feature_matrix_train[i:i+B,:], coefficients)\n",
    "indicator = (sentiment_train[i:i+B]==+1)\n",
    "\n",
    "errors = indicator - predictions        \n",
    "gradient_mini_batch = feature_derivative_with_L2(errors, feature_matrix_train[i:i+B, j], \n",
    "                                    coefficients[j], l2_penalty, False)\n",
    "print \"Gradient mini-batch data points: %s\" % gradient_mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** The code block above computed \n",
    "$\\color{red}{\\sum_{s = i}^{i+B}}\\frac{\\partial\\ell_{s}(\\mathbf{w})}{{\\partial w_j}}$ \n",
    "for `j = 10`, `i = 10`, and `B = 10`. Is this a scalar or a 194-dimensional vector?\n",
    "\n",
    "\n",
    "** Quiz Question:** For what value of `B` is the term\n",
    "$\\color{red}{\\sum_{s = 1}^{B}}\\frac{\\partial\\ell_{s}(\\mathbf{w})}{\\partial w_j}$\n",
    "the same as the full gradient\n",
    "$\\frac{\\partial\\ell(\\mathbf{w})}{{\\partial w_j}}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging the gradient across batch-size\n",
    "\n",
    "It is a common practice to **normalize** the gradient update rule by the batch size B:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell_{\\color{red}{A}}(\\mathbf{w})}{\\partial w_j} \\approx \\color{red}{\\frac{1}{B}} {\\sum_{s = i}^{i + B}} h_j(\\mathbf{x}_s)\\left(\\mathbf{1}[y_s = +1] - P(y_s = +1 | \\mathbf{x}_s, \\mathbf{w})\\right) - 2\\lambda w_j\n",
    "$$\n",
    "In other words, we update the coefficients using the **average gradient over data points** (instead using a summation). By using the average gradient, we ensure many things:\n",
    "* First, the L2 penalty $\\lambda$ has consistent effect regardless of the choice of the batch size.\n",
    "* Second, we can compare various batch sizes of stochastic-gradient ascent (including a batch-size of **all the data points**) and study the effect of batch size on the algorithm.\n",
    "\n",
    "\n",
    "## Implementing stochastic-gradient ascent\n",
    "\n",
    "Now we are ready to implement our own logistic regression with stochastic-gradient ascent. Complete the following function to solve the logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def logistic_regression_SG(feature_matrix, sentiment, initial_coefficients, step_size, batch_size, l2_penalty, max_iter):\n",
    "    log_likelihood_all = []\n",
    "    \n",
    "    # make sure it's a numpy array\n",
    "    coefficients = np.array(initial_coefficients)\n",
    "    # set seed=0 to produce consistent results\n",
    "    np.random.seed(seed=0)\n",
    "    \n",
    "    for itr in xrange(max_iter):\n",
    "        # Randomly select an index\n",
    "        i = np.random.randint(len(feature_matrix)-batch_size+1)\n",
    "        \n",
    "        # Predict P(y_i = +1|x_1,w) using your predict_probability() function\n",
    "        # Make sure to slice the i-th row of feature_matrix with [i:i+batch_size,:]\n",
    "        ### YOUR CODE HERE\n",
    "        predictions = ...\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        # Make sure to slice the i-th entry with [i:i+batch_size]\n",
    "        indicator = (sentiment[i:i+batch_size]==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # compute the derivative for coefficients[j].\n",
    "            # Make sure to slice the i-th row of feature_matrix with [i:i+batch_size,j]\n",
    "            # For the last argument, test if j is 0 or not.\n",
    "            ### YOUR CODE HERE\n",
    "            derivative = ...\n",
    "            \n",
    "            # compute the product of the step size, the derivative, and the **normalization constant** (1./batch_size)\n",
    "            ### YOUR CODE HERE\n",
    "            coefficients[j] += ...\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        # Print the log likelihood over the *current batch*\n",
    "        lp = compute_avg_log_likelihood_with_l2(feature_matrix[i:i+batch_size,:], sentiment[i:i+batch_size],\n",
    "                                                coefficients, l2_penalty)\n",
    "        log_likelihood_all.append(lp)\n",
    "        if itr <= 15 or (itr <= 1000 and itr % 100 == 0) or (itr <= 10000 and itr % 1000 == 0) \\\n",
    "         or itr % 10000 == 0 or itr == max_iter-1:\n",
    "            data_size = len(feature_matrix)\n",
    "            print 'Iteration %*d: Average log likelihood (of data points in batch [%0*d:%0*d]) = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, \\\n",
    "                 int(np.ceil(np.log10(data_size))), i, \\\n",
    "                 int(np.ceil(np.log10(data_size))), i+batch_size, lp)\n",
    "                \n",
    "    # We return the list of log likelihoods for plotting purposes.\n",
    "    return coefficients, log_likelihood_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "\n",
    "\n",
    "The following cell tests your stochastic-gradient ascent function using a toy dataset consisting of two data points. If the test does not pass, make sure you are normalizing the gradient update rule correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_feature_matrix = np.array([[1.,2.,-1.], [1.,0.,1.]])\n",
    "sample_sentiment = np.array([+1, -1])\n",
    "\n",
    "coefficients, log_likelihood = logistic_regression_SG(sample_feature_matrix, sample_sentiment, np.zeros(3),\n",
    "                                                  step_size=1., batch_size=2, l2_penalty=0, max_iter=2)\n",
    "print '-------------------------------------------------------------------------------------'\n",
    "print 'Coefficients compute                 :', coefficients\n",
    "print 'Average log likelihood per-iteration :', log_likelihood\n",
    "if np.allclose(coefficients, np.array([-0.09755757,  0.68242552, -0.7799831]), atol=1e-3)\\\n",
    "  and np.allclose(log_likelihood, np.array([-0.33774513108142956, -0.2345530939410341])):\n",
    "    # pass if elements match within 1e-3\n",
    "    print '-------------------------------------------------------------------------------------'\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print '-------------------------------------------------------------------------------------'\n",
    "    print 'Test failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare convergence behavior of stochastic-gradient ascent\n",
    "\n",
    "For the remainder of the assignment, we will try and compare compares stochastic-gradient ascent against gradient ascent. For that we need a reference stochastic-gradient ascent implementation. But do we need to implement this from scratch, do we?\n",
    "\n",
    "**Quiz Question:** For what value of batch-size **B** above is the stochastic-gradient ascent function `logistic_regression_SG` the same as gradient ascent from the previous assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running gradient ascent using the stochastic-gradient ascent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing them separately, we save time by re-using the stochastic-gradient ascent function we just wrote &mdash; **to perform gradient ascent**, it suffices to set **`batch_size`** to the number of data points in the training data. Yes, we did answer above the quiz question for you, but that is an important point to remember in the future.\n",
    "\n",
    "**Small Caveat**. The batch gradient ascent implementation here is slightly different than the one in the earlier assignments, as we now normalize the gradient update rule.\n",
    "\n",
    "We now **run stochastic gradient ascent** over the **feature_matrix_train** for 10 iterations using:\n",
    "* `initial_coefficients = np.zeros(194)`\n",
    "* `step_size = 5e-1`\n",
    "* `batch_size = 1`\n",
    "* `l2_penalty = 0.0`\n",
    "* `max_iter = 11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefficients, log_likelihood = logistic_regression_SG(feature_matrix_train, sentiment_train,\n",
    "                                        initial_coefficients=np.zeros(194),\n",
    "                                        step_size=5e-1, batch_size=1, l2_penalty=0.,\n",
    "                                        max_iter=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. When you set `batch_size = 1`, as each iteration passes, how does the average log-likelihood change:\n",
    "* Increases\n",
    "* Descreases\n",
    "* Fluctuates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run **batch gradient ascent** over the **feature_matrix_train** for 200 iterations using:\n",
    "* `initial_coefficients = np.zeros(194)`\n",
    "* `step_size = 5e-1`\n",
    "* `batch_size = len(feature_matrix_train)`\n",
    "* `l2_penalty = 0.0`\n",
    "* `max_iter = 200`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefficients_batch, log_likelihood_batch = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. When you set `batch_size = len(train_data)`, as each iteration passes, how does the average likelihood change.\n",
    "* Increases \n",
    "* Descreases\n",
    "* Fluctuates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make \"passes\" over the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a fair comparison betweeen stochastic-gradient ascent and batch-gradient ascent, we use measure the average log-likelihood as a function of the number of passes (defined as follows):\n",
    "$$\n",
    "[\\text{# of passes}] = \\frac{[\\text{# of data points touched so far}]}{[\\text{size of dataset}]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question** Suppose that we run stochastic-gradient ascent with a batch size of 100. How many gradient updates are performed at the end of two passes over a dataset consisting of 50000 data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihood plots for stochastic-gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the terminology in mind, let us run stochastic-gradient ascent for 10 passes. We will use\n",
    "* `step_size=1e-1`\n",
    "* `batch_size=100`\n",
    "* `l2_penalty=0`. \n",
    "* `initial_coefficients` to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step_size = 1e-1\n",
    "batch_size = 100\n",
    "num_passes = 2\n",
    "num_iterations = num_passes * int(len(feature_matrix_train)/batch_size)\n",
    "\n",
    "coefficients_sgd_first_2, log_likelihood_sgd_first_2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you a utility function to plot how the average log-likelihood changes as the number of passes increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plot_init = True\n",
    "\n",
    "def make_plot(log_likelihood_all, len_data, batch_size, smoothing_window=1, label=''):\n",
    "    plt.rcParams.update({'figure.figsize': (9,5)})\n",
    "    log_likelihood_all_ma = np.convolve(np.array(log_likelihood_all), \\\n",
    "                                        np.ones((smoothing_window,))/smoothing_window, mode='valid')\n",
    "    plt.plot(np.array(range(smoothing_window-1, len(log_likelihood_all)))*float(batch_size)/len_data,\n",
    "             log_likelihood_all_ma, linewidth=4.0, label=label)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('# of passes over data')\n",
    "    plt.ylabel('Average log likelihood per data point')\n",
    "    plt.legend(loc='lower right', prop={'size':14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_plot(log_likelihood_sgd_first_2, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          label='stochastic-gradient, step_size=1e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing the stochastic-gradient ascent curve\n",
    "\n",
    "The plotted line oscillates so much that it is hard to see whether the log likelihood is improving. In our plot, we apply a simple smoothing operation using the parameter `smoothing_window`. The smoothing is simply a [moving average](https://en.wikipedia.org/wiki/Moving_average) of log-likelihood over the last `smoothing_window` \"iteraetions\" of  stochastic-gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_plot(log_likelihood_sgd_first_2, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          smoothing_window=30, label='stochastic-gradient, step_size=1e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: The above plot should look smoother than the previous plot. Play around with `smoothing_window`. As you increase it, you should see a smoother plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic-gradient ascent vs gradient ascent\n",
    "\n",
    "To compare convergence rates for stochastic-gradient ascent with gradient ascent, we call `make_plot()` multiple times in the same cell.\n",
    "\n",
    "We are comparing:\n",
    "* **stochastic-gradient ascent**: `step_size = 0.1`, `batch_size=100`\n",
    "* **batch-gradient ascent**: `step_size = 0.5`, `batch_size=len(feature_matrix_train)`\n",
    "\n",
    "Write code to run stochastic-gradient ascent for 200 passes using:\n",
    "* `step_size=1e-1`\n",
    "* `batch_size=100`\n",
    "* `l2_penalty=0`. \n",
    "* `initial_coefficients` to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step_size = 1e-1\n",
    "batch_size = 100\n",
    "num_passes = 200\n",
    "num_iterations = num_passes * int(len(feature_matrix_train)/batch_size)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "coefficients_sgd, log_likelihood_sgd = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_plot(log_likelihood_sgd, len_data=len(feature_matrix_train), batch_size=100,\n",
    "          smoothing_window=30, label='stochastic, step_size=1e-1')\n",
    "make_plot(log_likelihood_batch, len_data=len(feature_matrix_train), batch_size=len(feature_matrix_train),\n",
    "          smoothing_window=1, label='batch, step_size=5e-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: In the figure above, how many passes does batch-gradient ascent need to achieve a similar log likelihood as stochastic-gradient ascent? \n",
    "\n",
    "1. Its always better\n",
    "2. 10 passes\n",
    "3. 20 passes\n",
    "4. 200 passes or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the effects of step sizes on stochastic-gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections, we chose step sizes for you. In practice, it helps to know how to choose good step sizes yourself.\n",
    "\n",
    "To start, we explore a wide range of step sizes that are equally space in the log space. Run stochastic-gradient ascent for `num_passes = 10` using `step_size` set to 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, and 1e2. Use \n",
    "* `initial_coefficients=np.zeros(194)`\n",
    "* `batch_size=100`\n",
    "* `l2_penalty=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_passes = 10\n",
    "num_iterations = num_passes * int(len(train_data)/batch_size)\n",
    "\n",
    "coefficients_sgd = {}\n",
    "log_likelihood_sgd = {}\n",
    "for step_size in np.logspace(-4, 2, num=7):\n",
    "    coefficients_sgd[step_size], log_likelihood_sgd[step_size] \\\n",
    "        = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the log-likelihood as a function of passes for each step size\n",
    "\n",
    "Now, we will plot the change in log-likelihood using the `make_plot` for each of the following values of `step_size`:\n",
    "\n",
    "* `step_size = 1e-4`\n",
    "* `step_size = 1e-3`\n",
    "* `step_size = 1e-2`\n",
    "* `step_size = 1e-1`\n",
    "* `step_size = 1e0`\n",
    "* `step_size = 1e1`\n",
    "* `step_size = 1e2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for step_size in np.logspace(-4, 2, num=7):\n",
    "    make_plot(log_likelihood_sgd[step_size], len_data=len(train_data), batch_size=100,\n",
    "              smoothing_window=30, label='step_size=%.1e'%step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question**: Did `step_size = 1e2` diverge or converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let ue remove the step size `step_size = 1e2` and plot the rest of the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for step_size in np.logspace(-4, 2, num=7)[0:6]:\n",
    "    make_plot(log_likelihood_sgd[step_size], len_data=len(train_data), batch_size=100,\n",
    "              smoothing_window=30, label='step_size=%.1e'%step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: For which of the following step sizes has stochastic-gradient ascent be said to diverge? Choose all that apply. Hint: Which of the plotted lines fail to approach the optimum?\n",
    "1. 1e-2\n",
    "2. 1e-1\n",
    "3. 1e0\n",
    "4. 1e1\n",
    "5. 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
